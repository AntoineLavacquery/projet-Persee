{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fichier pris en entrée :\n",
    "- *.csv -> provient du notebook **nettoyeur**\n",
    "\n",
    "Fichiers de sortie produits par le notebook :\n",
    "- NLP_{date}_{type_pipeline}_{nombre_resultats}.html -> **fichier HTML des résultats de la NER**\n",
    "\n",
    "- NLP_{date}_{type_pipeline}_{nombre_resultats}.csv -> **tableau avec autant de lignes que de résultats et deux colonnes : titre et pers**\n",
    "**-> pers** = entitées nommées \"personnage\" issues de spacy pour lesquelles un traitement de nettoyage a été effectué :\n",
    "    - suppression des termes inférieur à 2 caractères\n",
    "    - suppression des doublons malgré d'éventuelles coquilles d'OCR dans la graphie du nom -> la graphie retenue est celle qui revient le plus de fois au sein du CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:38:39 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1ca6c95a7d44f1b7dcf6d30d7285f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:38:40 WARNING: Language fr package default expects mwt, which has been added\n",
      "2023-02-06 17:38:41 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| ner       | wikiner |\n",
      "=======================\n",
      "\n",
      "2023-02-06 17:38:41 INFO: Use device: cpu\n",
      "2023-02-06 17:38:41 INFO: Loading: tokenize\n",
      "2023-02-06 17:38:41 INFO: Loading: mwt\n",
      "2023-02-06 17:38:41 INFO: Loading: ner\n",
      "2023-02-06 17:38:42 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import fr_core_news_lg # md, sm\n",
    "nlp_spacy = fr_core_news_lg.load()\n",
    "import json\n",
    "import itertools\n",
    "import regex as re\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import stanza\n",
    "nlp_stanza = stanza.Pipeline(lang='fr', processors='tokenize,ner')\n",
    "# # NOT WORKING ON APPLE SILICON\n",
    "# from flair.data import Sentence\n",
    "# from flair.models import SequenceTagger\n",
    "# tagger = SequenceTagger.load(\"flair/ner-french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables à changer par l'utilisateur\n",
    "source_csv = 'results/clean_w_names_24-01-23.csv'\n",
    "pipeline = \"fr_core_news_lg\" # /!\\ PENSER À CHANGER PLUS HAUT\n",
    "\n",
    "# Importation des données depuis le CSV\n",
    "df = pd.read_csv(source_csv)\n",
    "\n",
    "# Nombre de résultats dans la réponse produite, varie en fonction des essais\n",
    "# nb_a_traiter = len(df.index)\n",
    "nb_a_traiter = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "lists = []\n",
    "\n",
    "# Boucle de constitution\n",
    "for i in range(nb_a_traiter):\n",
    "    title = df.loc[i, 'area_title']\n",
    "    text = df.loc[i, 'area_text']\n",
    "    sign_name = df.loc[i, 'extracted_names']\n",
    "\n",
    "    # NER spacy\n",
    "    doc_spacy = nlp_spacy(text)\n",
    "\n",
    "    # NER stanza\n",
    "    doc_stanza = nlp_stanza(text)\n",
    "\n",
    "    # # NER flair\n",
    "    # wip ?\n",
    "\n",
    "    # Constitution de la grande liste destinée à être convertie en df ------------------------\n",
    "    spacy = [(ent.text) for ent in doc_spacy.ents if ent.label_ == \"PER\"]\n",
    "    stanza = [(ent.text) for ent in doc_stanza.ents if ent.type == \"PER\"]\n",
    "    list = [title, sign_name, spacy, stanza]\n",
    "    lists.append(list)\n",
    "\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de la liste vers df\n",
    "df_PER = pd.DataFrame(lists, columns=['title', 'sign_name', 'spacy', 'stanza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de nettoyage de la liste des entitées reconnues\n",
    "def clean_names(list):\n",
    "    patterns = [\n",
    "        r'^M\\.',\n",
    "        r'^MM\\.',\n",
    "        r'^MMe.',\n",
    "        r'^MMe ',\n",
    "        r'^ +',\n",
    "    ]\n",
    "    for i in range(len(list)):\n",
    "        for pattern in patterns:\n",
    "            list[i] = re.sub(pattern, '', list[i])\n",
    "    return(list)\n",
    "\n",
    "# Comptage des termes au sein de la liste\n",
    "def make_count(list):\n",
    "    list_count = []\n",
    "    for i in range(len(list)):\n",
    "        mot = list[i]\n",
    "        count = list.count(list[i])\n",
    "        list_count.append([mot, count])\n",
    "    return(list_count)\n",
    "\n",
    "# Fonction de comparaisons des mots 2 à 2 : lorsque plusieurs noms sont similaires, on n'en garde qu'un\n",
    "# Fonction qui prend une liste en entrée et ressort une liste en sortie\n",
    "def keep_best_name(list):\n",
    "    discri_list = [\n",
    "        r'^[\\P{Lu}]',\n",
    "        r'[0-9]+',\n",
    "        r'\\.$',\n",
    "        r'\\\"|\\'|\\\\|\\/|«|»'\n",
    "    ]\n",
    "\n",
    "    # Pour chaque mot\n",
    "    temp_list = []\n",
    "    for i in range(len(list)):\n",
    "        ref = list[i]\n",
    "\n",
    "        # On va comparera avec tous autres mots de la liste\n",
    "        for j in range(len(list)):\n",
    "            # Si on confronte le mot avec lui même -> on ne fait rien\n",
    "            if i == j:\n",
    "                pass\n",
    "            else:\n",
    "                # Sinon comparaison du score entre les deux noms\n",
    "                ratio = fuzz.token_sort_ratio(ref[0], list[j][0])\n",
    "                partial_ratio = fuzz.partial_ratio(ref[0], list[j][0])\n",
    "                if ratio > 85 or partial_ratio > 85:\n",
    "                    # Si le second mot est présent plus de fois, on le retient lui\n",
    "                    if ref[1] < list[j][1]:\n",
    "                        ref = list[j]\n",
    "\n",
    "        # Discrimination des noms de 2 lettres ou moins, ceux qui contiennent de chiffres, qui ne commencent pas par une maj et ceux ne contenant pas de voyelles\n",
    "        if (ref[0] not in temp_list) & (len(ref[0]) > 2):\n",
    "            ready = True\n",
    "            for pattern in discri_list:\n",
    "                if re.search(pattern, ref[0]):\n",
    "                    ready = False\n",
    "                if not re.search('(?i)[aeiouy]+', ref[0]):\n",
    "                    ready = False\n",
    "            if ready:\n",
    "                temp_list.append(ref[0])\n",
    "    return(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_names(list_of_names):\n",
    "    # On appelle la fonction de nettoyage sur nos entitées \"personnes\"\n",
    "    list_PER_clean = clean_names(list_of_names)\n",
    "    # Chaque entitée est compté, un nombre d'occurence lui est attribué = [['Personne1', 2], ['Personne2', 3], ['Personne3', 1], ['Personne4', 1]]\n",
    "    list_PER_count = make_count(list_PER_clean)\n",
    "    # De tous les noms qui se ressemblent (= coquilles dans la graphie), on ne garde que l'occurence qui est apparue le plus de fois\n",
    "    best_names = keep_best_name(list_PER_count)\n",
    "    return(best_names)\n",
    "\n",
    "# Pour chaque ligne de notre tableau de départ\n",
    "for i in range(len(df_PER.index)):\n",
    "    df_PER.at[i, 'spacy'] = process_names(df_PER.at[i, 'spacy'])\n",
    "    df_PER.at[i, 'stanza'] = process_names(df_PER.at[i, 'stanza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Re)création d'un fichier de sortie propre + en tête\n",
    "nom_fichier = f\"results/ner_{nb_a_traiter}r\"\n",
    "\n",
    "# Conversion du df vers csv\n",
    "df_PER.to_csv(f\"{nom_fichier}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
