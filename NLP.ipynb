{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fichier pris en entrée :\n",
    "- clean_{date}.json -> provient du notebook **nettoyeur**\n",
    "\n",
    "Fichiers de sortie produits par le notebook :\n",
    "- NLP_{date}_{type_pipeline}_{nombre_resultats}.html -> **fichier HTML des résultats de la NER**\n",
    "\n",
    "- NLP_{date}_{type_pipeline}_{nombre_resultats}.csv -> **tableau avec autant de lignes que de résultats et deux colonnes : titre et pers**\n",
    "**-> pers** = entitées nommées \"personnage\" issues de spacy pour lesquelles un traitement de nettoyage a été effectué :\n",
    "    - suppression des termes inférieur à 2 caractères\n",
    "    - suppression des doublons malgré d'éventuelles coquilles d'OCR dans la graphie du nom -> la graphie retenue est celle qui revient le plus de fois au sein du CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "# Pipelines au choix small, medium, large (du - au + précis)\n",
    "# import fr_core_news_sm\n",
    "# import fr_core_news_md\n",
    "import fr_core_news_lg\n",
    "nlp = fr_core_news_lg.load()\n",
    "from datetime import date\n",
    "import time\n",
    "import json\n",
    "import itertools\n",
    "import regex as re\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables à changer par l'utilisateur\n",
    "source_json = 'results/clean_w_names_24-01-23.json'\n",
    "pipeline = \"fr_core_news_lg\" # /!\\ PENSER À CHANGER PLUS HAUT\n",
    "\n",
    "# Importation des données depuis le JSON\n",
    "df = pd.read_json(source_json, orient='index')\n",
    "\n",
    "# Nombre de résultats dans la réponse produite, varie en fonction des essais\n",
    "nb_a_traiter = len(df.index)\n",
    "# nb_a_traiter = 100\n",
    "\n",
    "# Variable permettant de compter puis d'inscrire le nombre de \"personnes\" détéctées par la NER dans le nom du fichier de sortie final (pour contrôle)\n",
    "count_names = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de l'en tête général\n",
    "date = time.strftime(\"%d-%m-%y\")\n",
    "\n",
    "# HTML description des labels\n",
    "labels_description = \"\"\"\n",
    "<p><strong>PERSON:</strong> People, including fictional.&emsp;<strong>NORP:</strong> Nationalities or religious or political groups.&emsp;\n",
    "<strong>FAC:</strong> Buildings, airports, highways, bridges, etc.&emsp;<strong>ORG:</strong> Companies, agencies, institutions, etc.&emsp;\n",
    "<strong>GPE:</strong> Countries, cities, states.&emsp;<strong>LOC:</strong> Non-GPE locations, mountain ranges, bodies of water.&emsp;\n",
    "<strong>PRODUCT:</strong> Objects, vehicles, foods, etc. (Not services.)&emsp;<strong>EVENT:</strong> Named hurricanes, battles, wars, sports events, etc.&emsp;\n",
    "<strong>WORK_OF_ART:</strong> Titles of books, songs, etc.&emsp;<strong>LAW:</strong> Named documents made into laws.&emsp;<strong>LANGUAGE:</strong> Any named language.&emsp;\n",
    "<strong>DATE:</strong> Absolute or relative dates or periods.&emsp;<strong>TIME:</strong> Times smaller than a day.&emsp;<strong>PERCENT:</strong> Percentage, including \"%\".&emsp;\n",
    "<strong>MONEY:</strong> Monetary values, including unit.&emsp;<strong>QUANTITY:</strong> Measurements, as of weight or distance.&emsp;<strong>ORDINAL:</strong> \"first\", \"second\", etc.&emsp;\n",
    "<strong>CARDINAL:</strong> Numerals that do not fall under another type.</p>\n",
    "\"\"\"\n",
    "\n",
    "heading = f\"\"\"\n",
    "    <p><strong>date:</strong> {date}</p>\n",
    "    <p><strong>source:</strong> {source_json}</p>\n",
    "    <p><strong>pipeline:</strong> {pipeline}</p>\n",
    "    <p><strong>quantity processessed:</strong> {nb_a_traiter}</p>\n",
    "    <hr>\n",
    "    {labels_description}\n",
    "    \"\"\"\n",
    "\n",
    "# (Re)création d'un fichier de sortie propre + en tête\n",
    "nom_fichier = f\"results/NLP_{date}_{pipeline[-2:]}_{nb_a_traiter}r_{count_names}n\"\n",
    "\n",
    "# with open(f\"{nom_fichier}.html\", \"w\") as fichier:\n",
    "#     fichier.write(heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personnalisation des couleurs du rendu\n",
    "colors = {\n",
    "    \"PER\": \"#97C7E8\",\n",
    "    \"ORG\": \"#A4DBA4\",\n",
    "    \"GPE\": \"#F2937C\",\n",
    "    \"LOC\": \"#AE9DF2\",\n",
    "    \"EVENT\": \"#E8BC76\",\n",
    "    \"WORK_OF_ART\": \"#DB99DB\",\n",
    "    \"MISC\": \"#F2A99D\",\n",
    "    \"DATE\": \"#A7F2BD\",\n",
    "    \"ORDINAL\": \"#E8D3A2\",\n",
    "    \"CARDINAL\": \"#E8D3A2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = []\n",
    "\n",
    "# Boucle de constitution : export PER + HTML\n",
    "# for i in range(nb_a_traiter): # range(len(df.index))\n",
    "for i in range(nb_a_traiter):\n",
    "    # NLP par spacy --------------------------------------------------------------------------\n",
    "    title = df.loc[i, 'area_title']\n",
    "    text = df.loc[i, 'area_text']\n",
    "    extracted_names = df.loc[i, 'extracted_names']\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Constitution de la grande liste destinée à être convertie en df ------------------------\n",
    "    pers = [(ent.text) for ent in doc.ents if ent.label_ == \"PER\"]\n",
    "    list = [title, pers, extracted_names]\n",
    "    lists.append(list)\n",
    "\n",
    "    # Constitution de l'HTML de contrôle du NLP ----------------------------------------------\n",
    "    sentence_tokens = len([[token.text for token in sent] for sent in doc.sents])\n",
    "\n",
    "    # # Génération du rendu displacy\n",
    "    # html = displacy.render(doc, style=\"ent\", jupyter=False, page=True, options={\"colors\": colors})\n",
    "\n",
    "    # # Définition de l'en tête pour chaque résultat\n",
    "    # headings = f\"\"\"\n",
    "    # <hr>\n",
    "    # <p><strong>index:</strong> {i}</p>\n",
    "    # <p><strong>title:</strong> {title}</p>\n",
    "    # <p><strong>number of sentences:</strong> {sentence_tokens}</p>\n",
    "    # \"\"\"\n",
    "\n",
    "    # # Inscription de l'en tête + inscription de résultat\n",
    "    # with open(f\"{nom_fichier}.html\", 'a') as fichier:\n",
    "    #     fichier.write(headings)\n",
    "    #     fichier.write(str(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de la liste vers df\n",
    "df_PER = pd.DataFrame(lists, columns=['title', 'pers', 'extracted_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = df_PER['pers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de nettoyage de la liste des entitées reconnues\n",
    "def clean_names(list):\n",
    "    patterns = [\n",
    "        r'^M\\.',\n",
    "        r'^MM\\.',\n",
    "        r'^MMe.',\n",
    "        r'^MMe ',\n",
    "        r'^ +',\n",
    "    ]\n",
    "    for i in range(len(list)):\n",
    "        for pattern in patterns:\n",
    "            list[i] = re.sub(pattern, '', list[i])\n",
    "    return(list)\n",
    "\n",
    "# Comptage des termes au sein de la liste\n",
    "def make_count(list):\n",
    "    list_count = []\n",
    "    for i in range(len(list)):\n",
    "        mot = list[i]\n",
    "        count = list.count(list[i])\n",
    "        list_count.append([mot, count])\n",
    "    return(list_count)\n",
    "\n",
    "# Fonction de comparaisons des mots 2 à 2 : lorsque plusieurs noms sont similaires, on n'en garde qu'un\n",
    "# Fonction qui prend une liste en entrée et ressort une liste en sortie\n",
    "def keep_best_name(list):\n",
    "    discri_list = [\n",
    "        r'^[\\P{Lu}]',\n",
    "        r'[0-9]+',\n",
    "        r'\\.$',\n",
    "        r'\\\"|\\'|\\\\|\\/|«|»'\n",
    "    ]\n",
    "\n",
    "    # Pour chaque mot\n",
    "    temp_list = []\n",
    "    for i in range(len(list)):\n",
    "        ref = list[i]\n",
    "\n",
    "        # On va comparera avec tous autres mots de la liste\n",
    "        for j in range(len(list)):\n",
    "            # Si on confronte le mot avec lui même -> on ne fait rien\n",
    "            if i == j:\n",
    "                pass\n",
    "            else:\n",
    "                # Sinon comparaison du score entre les deux noms\n",
    "                ratio = fuzz.token_sort_ratio(ref[0], list[j][0])\n",
    "                partial_ratio = fuzz.partial_ratio(ref[0], list[j][0])\n",
    "                if ratio > 85 or partial_ratio > 85:\n",
    "                    # Si le second mot est présent plus de fois, on le retient lui\n",
    "                    if ref[1] < list[j][1]:\n",
    "                        ref = list[j]\n",
    "\n",
    "        # Discrimination des noms de 2 lettres ou moins, ceux qui contiennent de chiffres, qui ne commencent pas par une maj et ceux ne contenant pas de voyelles\n",
    "        if (ref[0] not in temp_list) & (len(ref[0]) > 2):\n",
    "            ready = True\n",
    "            for pattern in discri_list:\n",
    "                if re.search(pattern, ref[0]):\n",
    "                    ready = False\n",
    "                if not re.search('(?i)[aeiouy]+', ref[0]):\n",
    "                    ready = False\n",
    "            if ready:\n",
    "                temp_list.append(ref[0])\n",
    "    return(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour chaque ligne de notre tableau de départ\n",
    "for i in range(len(df_PER.index)):\n",
    "    # On appelle la fonction de nettoyage sur nos entitées \"personnes\"\n",
    "    list_PER_clean = clean_names(df_PER['pers'][i])\n",
    "    # Chaque entitée est compté, un nombre d'occurence lui est attribué = [['Personne1', 2], ['Personne2', 3], ['Personne3', 1], ['Personne4', 1]]\n",
    "    list_PER_count = make_count(list_PER_clean)\n",
    "    # De tous les noms qui se ressemblent (= coquilles dans la graphie), on ne garde que l'occurence qui est apparue le plus de fois\n",
    "    best_names = keep_best_name(list_PER_count)\n",
    "    # Décompte pour contrôle \n",
    "    count_names += len(best_names)\n",
    "    # Màj de notre liste d'entités \"personnes\" au sein du dataframe\n",
    "    df_PER.at[i, 'pers'] = best_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19448"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Re)création d'un fichier de sortie propre + en tête\n",
    "nom_fichier = f\"results/NLP_{date}_{pipeline[-2:]}_{nb_a_traiter}r_{count_names}n\"\n",
    "\n",
    "# Conversion du df vers csv\n",
    "df_PER.to_csv(f\"{nom_fichier}.csv\")\n",
    "\n",
    "count_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVANT : ['P. Vergili Maronis', 'Épigrammata', 'E. Galletier', 'Gailetier', 'Galletier', 'Suétone', 'Virgile', 'fecit Kax', 'Galletier', 'Domitien', 'Néron', 'Virgile', 'Galletier', 'Birt, Gubernatis', 'V', 'Vili', 'Ite hinc', 'Catulle X', 'Sabinus Me', 'Virgile', 'Varius', 'Octavius', 'IX', 'Virgile', 'Corinthiorum', 'Galletier', 'Virgile', 'Catulle', 'Virgile', 'Racine', 'Virgile', 'Galletier', 'Catulle', 'Galletier', 'Birt', 'Gubernatis', 'René Pichon']\n",
      "----------------------\n",
      "APRÈS : ['P. Vergili Maronis', 'Épigrammata', 'Galletier', 'Suétone', 'Virgile', 'Domitien', 'Néron', 'Birt, Gubernatis', 'Vili', 'Ite hinc', 'Catulle', 'Sabinus Me', 'Varius', 'Octavius', 'Corinthiorum', 'Racine', 'Birt', 'Gubernatis', 'René Pichon']\n"
     ]
    }
   ],
   "source": [
    "after = df_PER['pers'][0]\n",
    "print(f'AVANT : {before}')\n",
    "print('----------------------')\n",
    "print(f'APRÈS : {after}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
