{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP+"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fichier pris en entrée :\n",
    "- clean_{date}.json -> provient du notebook **nettoyeur**\n",
    "\n",
    "Fichiers de sortie produits par le notebook :\n",
    "- NLP_{date}_{type_pipeline}_{nombre_resultats}.html -> **fichier HTML des résultats de la NER**\n",
    "\n",
    "- NLP_{date}_{type_pipeline}_{nombre_resultats}.csv -> **tableau avec autant de lignes que de résultats et deux colonnes : titre et pers**\n",
    "**-> pers** = entitées nommées \"personnage\" issues de spacy pour lesquelles un traitement de nettoyage a été effectué :\n",
    "    - suppression des termes inférieur à 2 caractères\n",
    "    - suppression des doublons malgré d'éventuelles coquilles d'OCR dans la graphie du nom -> la graphie retenue est celle qui revient le plus de fois au sein du CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "# Pipelines au choix small, medium, large (du - au + précis)\n",
    "# import fr_core_news_sm\n",
    "# import fr_core_news_md\n",
    "import fr_core_news_lg\n",
    "nlp = fr_core_news_lg.load()\n",
    "from datetime import date\n",
    "import time\n",
    "import json\n",
    "import itertools\n",
    "import regex as re\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables à changer par l'utilisateur\n",
    "source_json = 'results/clean_w_names_24-01-23.json'\n",
    "pipeline = \"fr_core_news_lg\" # /!\\ PENSER À CHANGER PLUS HAUT\n",
    "\n",
    "# Importation des données depuis le JSON\n",
    "df = pd.read_json(source_json, orient='index')\n",
    "\n",
    "nb_a_traiter = len(df.index)\n",
    "# nb_a_traiter = 100\n",
    "\n",
    "count_names = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de l'en tête général\n",
    "date = time.strftime(\"%d-%m-%y\")\n",
    "\n",
    "# HTML description des labels\n",
    "labels_description = \"\"\"\n",
    "<p><strong>PERSON:</strong> People, including fictional.&emsp;<strong>NORP:</strong> Nationalities or religious or political groups.&emsp;\n",
    "<strong>FAC:</strong> Buildings, airports, highways, bridges, etc.&emsp;<strong>ORG:</strong> Companies, agencies, institutions, etc.&emsp;\n",
    "<strong>GPE:</strong> Countries, cities, states.&emsp;<strong>LOC:</strong> Non-GPE locations, mountain ranges, bodies of water.&emsp;\n",
    "<strong>PRODUCT:</strong> Objects, vehicles, foods, etc. (Not services.)&emsp;<strong>EVENT:</strong> Named hurricanes, battles, wars, sports events, etc.&emsp;\n",
    "<strong>WORK_OF_ART:</strong> Titles of books, songs, etc.&emsp;<strong>LAW:</strong> Named documents made into laws.&emsp;<strong>LANGUAGE:</strong> Any named language.&emsp;\n",
    "<strong>DATE:</strong> Absolute or relative dates or periods.&emsp;<strong>TIME:</strong> Times smaller than a day.&emsp;<strong>PERCENT:</strong> Percentage, including \"%\".&emsp;\n",
    "<strong>MONEY:</strong> Monetary values, including unit.&emsp;<strong>QUANTITY:</strong> Measurements, as of weight or distance.&emsp;<strong>ORDINAL:</strong> \"first\", \"second\", etc.&emsp;\n",
    "<strong>CARDINAL:</strong> Numerals that do not fall under another type.</p>\n",
    "\"\"\"\n",
    "\n",
    "heading = f\"\"\"\n",
    "    <p><strong>date:</strong> {date}</p>\n",
    "    <p><strong>source:</strong> {source_json}</p>\n",
    "    <p><strong>pipeline:</strong> {pipeline}</p>\n",
    "    <p><strong>quantity processessed:</strong> {nb_a_traiter}</p>\n",
    "    <hr>\n",
    "    {labels_description}\n",
    "    \"\"\"\n",
    "\n",
    "# (Re)création d'un fichier de sortie propre + en tête\n",
    "nom_fichier = f\"results/NLP_{date}_{pipeline[-2:]}_{nb_a_traiter}r_{count_names}n\"\n",
    "\n",
    "# with open(f\"{nom_fichier}.html\", \"w\") as fichier:\n",
    "#     fichier.write(heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personnalisation des couleurs du rendu\n",
    "colors = {\n",
    "    \"PER\": \"#97C7E8\",\n",
    "    \"ORG\": \"#A4DBA4\",\n",
    "    \"GPE\": \"#F2937C\",\n",
    "    \"LOC\": \"#AE9DF2\",\n",
    "    \"EVENT\": \"#E8BC76\",\n",
    "    \"WORK_OF_ART\": \"#DB99DB\",\n",
    "    \"MISC\": \"#F2A99D\",\n",
    "    \"DATE\": \"#A7F2BD\",\n",
    "    \"ORDINAL\": \"#E8D3A2\",\n",
    "    \"CARDINAL\": \"#E8D3A2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = []\n",
    "\n",
    "# Boucle de constitution : export PER + HTML\n",
    "# for i in range(nb_a_traiter): # range(len(df.index))\n",
    "for i in range(nb_a_traiter):\n",
    "    # NLP par spacy --------------------------------------------------------------------------\n",
    "    title = df.loc[i, 'area_title']\n",
    "    text = df.loc[i, 'area_text']\n",
    "    extracted_names = df.loc[i, 'extracted_names']\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Constitution de la grande liste destinée à être convertie en df ------------------------\n",
    "    pers = [(ent.text) for ent in doc.ents if ent.label_ == \"PER\"]\n",
    "    list = [title, pers, extracted_names]\n",
    "    lists.append(list)\n",
    "\n",
    "    # Constitution de l'HTML de contrôle du NLP ----------------------------------------------\n",
    "    sentence_tokens = len([[token.text for token in sent] for sent in doc.sents])\n",
    "\n",
    "    # # Génération du rendu displacy\n",
    "    # html = displacy.render(doc, style=\"ent\", jupyter=False, page=True, options={\"colors\": colors})\n",
    "\n",
    "    # # Définition de l'en tête pour chaque résultat\n",
    "    # headings = f\"\"\"\n",
    "    # <hr>\n",
    "    # <p><strong>index:</strong> {i}</p>\n",
    "    # <p><strong>title:</strong> {title}</p>\n",
    "    # <p><strong>number of sentences:</strong> {sentence_tokens}</p>\n",
    "    # \"\"\"\n",
    "\n",
    "    # # Inscription de l'en tête + inscription de résultat\n",
    "    # with open(f\"{nom_fichier}.html\", 'a') as fichier:\n",
    "    #     fichier.write(headings)\n",
    "    #     fichier.write(str(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de la liste vers df\n",
    "df_PER = pd.DataFrame(lists, columns=['title', 'pers', 'extracted_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de nettoyage\n",
    "def clean_names(list):\n",
    "    patterns = [\n",
    "        r'^M\\.',\n",
    "        r'^MM\\.',\n",
    "        r'^MMe.',\n",
    "        r'^MMe ',\n",
    "        r'^ +',\n",
    "    ]\n",
    "    for i in range(len(list)):\n",
    "        for pattern in patterns:\n",
    "            list[i] = re.sub(pattern, '', list[i])\n",
    "    return(list)\n",
    "\n",
    "# Fonction qui me prend une liste en entrée et ressort une liste en sortie\n",
    "def keep_best_name(list):\n",
    "    discri_list = [\n",
    "        r'^[\\P{Lu}]',\n",
    "        r'[0-9]+',\n",
    "        r'\\.$',\n",
    "        r'\\\"|\\'|\\\\|\\/|«|»'\n",
    "    ]\n",
    "\n",
    "    # Pour chaque mot\n",
    "    temp_list = []\n",
    "    for i in range(len(list)):\n",
    "        ref = list[i]\n",
    "\n",
    "        # On va comparera avec tous autres mots de la liste\n",
    "        for j in range(len(list)):\n",
    "            # Si on confronte le mot avec lui même -> on ne fait rien\n",
    "            if i == j:\n",
    "                pass\n",
    "            else:\n",
    "                ratio = fuzz.token_sort_ratio(ref[0], list[j][0])\n",
    "                partial_ratio = fuzz.partial_ratio(ref[0], list[j][0])\n",
    "                if ratio > 85:\n",
    "                    if ref[1] < list[j][1]:\n",
    "                        ref = list[j]\n",
    "                else:\n",
    "                    if partial_ratio > 85:\n",
    "                        if ref[1] < list[j][1]:\n",
    "                            ref = list[j]\n",
    "\n",
    "        \n",
    "        if (ref[0] not in temp_list) & (len(ref[0]) > 2):\n",
    "            ready = True\n",
    "            for pattern in discri_list:\n",
    "                if re.search(pattern, ref[0]):\n",
    "                    ready = False\n",
    "                if not re.search('(?i)[aeiouy]+', ref[0]):\n",
    "                    ready = False\n",
    "            if ready:\n",
    "                temp_list.append(ref[0])\n",
    "\n",
    "            # if not re.search(r'^[\\P{Lu}]', ref[0]):\n",
    "            #     temp_list.append(ref[0])\n",
    "    return(temp_list)\n",
    "\n",
    "# Comptage des termes au sein de la liste\n",
    "def make_count(list):\n",
    "    list_count = []\n",
    "    for i in range(len(list)):\n",
    "        mot = list[i]\n",
    "        count = list.count(list[i])\n",
    "        list_count.append([mot, count])\n",
    "    return(list_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extraction d'une liste de PER à partir du df précedemment constitué\n",
    "# list_PER = df_PER['pers'][13]\n",
    "# list_PER_clean = keep_best_name(list_PER)\n",
    "# list_PER_count = make_count(list_PER_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_PER.index)):\n",
    "    list_PER_clean = clean_names(df_PER['pers'][i])\n",
    "    list_PER_count = make_count(list_PER_clean)\n",
    "\n",
    "    best_names = keep_best_name(list_PER_count)\n",
    "    count_names += len(best_names)\n",
    "    df_PER.at[i, 'pers'] = best_names\n",
    "    # print(df_PER['pers'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19448"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Re)création d'un fichier de sortie propre + en tête\n",
    "nom_fichier = f\"results/NLP_{date}_{pipeline[-2:]}_{nb_a_traiter}r_{count_names}n\"\n",
    "\n",
    "# Conversion du df vers csv\n",
    "df_PER.to_csv(f\"{nom_fichier}.csv\")\n",
    "\n",
    "count_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
